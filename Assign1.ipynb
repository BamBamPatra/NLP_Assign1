{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries\n",
    "\n",
    "- `nltk`: For natural language processing tasks, such as tokenization and stopword removal.\n",
    "- `re`: For cleaning text using regular expressions.\n",
    "- `collections.Counter`: For counting word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bampatra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bampatra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure the necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text\n",
    "\n",
    "1. clean_text function:\n",
    "Converts text to lowercase, removes punctuation, numbers, extra spaces, and stopwords, then joins the cleaned words back into a string.\n",
    "\n",
    "2. tokenize_text_into_sentences_and_words function:\n",
    "Splits text into sentences, then tokenizes each sentence into words.\n",
    "\n",
    "3. Main workflow:\n",
    "Reads alice29.txt, cleans the text, and writes the cleaned text to cleaned.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! The cleaned text has been written to 'cleaned.txt'.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize the text (split into words)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Join the cleaned tokens back into a string\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize_text_into_sentences_and_words(text):\n",
    "    # Tokenize the cleaned text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    sentence_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "# Read the content of alice29.txt\n",
    "with open('resource/alice29.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Clean the text\n",
    "cleaned_text = clean_text(text)\n",
    "\n",
    "# Write the cleaned text to cleaned.txt\n",
    "with open('cleaned.txt', 'w') as cleaned_file:\n",
    "    cleaned_file.write(cleaned_text)\n",
    "\n",
    "print(\"Cleaning complete! The cleaned text has been written to 'cleaned.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing it into sentences and words\n",
    "\n",
    "1. Tokenize cleaned text: The code tokenizes the cleaned text into sentences and words using tokenize_text_into_sentences_and_words.\n",
    "\n",
    "2. Write tokens to file: It writes each word from the tokenized sentences to word.txt, with each word on a new line.\n",
    "\n",
    "3. Completion message: It prints a message confirming that the word tokenization is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete! The words have been written to 'word.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the cleaned text into sentences and words\n",
    "sentence_words = tokenize_text_into_sentences_and_words(cleaned_text)\n",
    "\n",
    "# Write the word tokens into word.txt\n",
    "with open('word.txt', 'w') as word_file:\n",
    "    for sentence in sentence_words:\n",
    "        for word in sentence:\n",
    "            word_file.write(word + '\\n')\n",
    "\n",
    "print(\"Tokenization complete! The words have been written to 'word.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic frequency analysis to identify the most common words in the text.\n",
    "\n",
    "1. tokenize_text_into_sentences_and_words: Tokenizes the cleaned text into sentences, then splits each sentence into words.\n",
    "\n",
    "2. perform_frequency_analysis: Counts the frequency of each word in the list of tokens using Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! The cleaned text has been written to 'cleaned.txt'.\n",
      "Frequency analysis complete! The top 10 words have been written to 'top_10_words.txt'.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenize the text (split into words)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Return the cleaned text (joined as a string of words)\n",
    "    return cleaned_tokens\n",
    "\n",
    "def tokenize_text_into_sentences_and_words(text):\n",
    "    # Tokenize the cleaned text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    sentence_words = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "def perform_frequency_analysis(tokens):\n",
    "    # Count the frequency of each word using Counter\n",
    "    word_counts = Counter(tokens)\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "# Read the content of alice29.txt\n",
    "try:\n",
    "    with open('resource/alice29.txt', 'r') as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(\"The file 'alice29.txt' was not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# Clean the text and get the list of cleaned tokens (words)\n",
    "cleaned_tokens = clean_text(text)\n",
    "\n",
    "# Write the cleaned tokens to cleaned.txt (each word on a new line)\n",
    "with open('cleaned.txt', 'w') as cleaned_file:\n",
    "    cleaned_file.write(' '.join(cleaned_tokens))\n",
    "\n",
    "print(\"Cleaning complete! The cleaned text has been written to 'cleaned.txt'.\")\n",
    "\n",
    "# Perform frequency analysis on the cleaned tokens\n",
    "word_counts = perform_frequency_analysis(cleaned_tokens)\n",
    "\n",
    "# Get the top 10 most common words and their counts\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# Write the top 10 words and their frequencies to top_10_words.txt\n",
    "with open('top_10_words.txt', 'w') as top_words_file:\n",
    "    for word, count in top_10_words:\n",
    "        top_words_file.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "print(\"Frequency analysis complete! The top 10 words have been written to 'top_10_words.txt'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
